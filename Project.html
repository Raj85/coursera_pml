<html>

<head>
<style type="text/css">
.knitr.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
},
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0em 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage.left {
  text-align: left;
}
.rimage.right {
  text-align: right;
}
.rimage.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<title>Practical Machine Learning</title>
</head>

<body>

<p>Data Processing

After downloading the data, load the data and replace all the missing values and other corrupt values to NA and then find which fields have maximum NA and ignore the fields </p>

<div class="chunk" id="unnamed-chunk-1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">trainData</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">read.csv</span><span class="hl std">(</span><span class="hl str">&quot;pml-training.csv&quot;</span><span class="hl std">,</span> <span class="hl kwc">header</span> <span class="hl std">=</span> <span class="hl num">TRUE</span><span class="hl std">,</span> <span class="hl kwc">na.strings</span><span class="hl std">=</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl str">&quot;&quot;</span><span class="hl std">,</span><span class="hl str">&quot;NA&quot;</span><span class="hl std">,</span> <span class="hl str">&quot;#DIV/0!&quot;</span><span class="hl std">))</span>
<span class="hl std">testData</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">read.csv</span><span class="hl std">(</span><span class="hl str">&quot;pml-testing.csv&quot;</span><span class="hl std">,</span> <span class="hl kwc">header</span> <span class="hl std">=</span> <span class="hl num">TRUE</span><span class="hl std">,</span> <span class="hl kwc">na.strings</span><span class="hl std">=</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl str">&quot;&quot;</span><span class="hl std">,</span><span class="hl str">&quot;NA&quot;</span><span class="hl std">,</span> <span class="hl str">&quot;#DIV/0!&quot;</span><span class="hl std">))</span>
</pre></div>
</div></div>

<p>Data cleaning</p>

<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">NAPercent</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">round</span><span class="hl std">(</span><span class="hl kwd">colMeans</span><span class="hl std">(</span><span class="hl kwd">is.na</span><span class="hl std">(trainData)),</span> <span class="hl num">2</span><span class="hl std">)</span>
<span class="hl kwd">table</span><span class="hl std">(NAPercent)</span>
</pre></div>
<div class="output"><pre class="knitr r">## NAPercent
##    0 0.98    1 
##   60   94    6
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">FieldPosition</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">which</span><span class="hl std">(NAPercent</span><span class="hl opt">==</span><span class="hl num">0</span><span class="hl std">)[</span><span class="hl opt">-</span><span class="hl num">1</span><span class="hl std">]</span>
<span class="hl com"># removal of missing data</span>
<span class="hl std">trainData</span> <span class="hl kwb">&lt;-</span> <span class="hl std">trainData[, FieldPosition]</span>
<span class="hl std">testData</span> <span class="hl kwb">&lt;-</span> <span class="hl std">testData[, FieldPosition]</span>
</pre></div>
</div></div>

<p>Converting the vairables to numeric anbd factor</p>

<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># Converting all the other dependant variables into numerical data to numeric class</span>

<span class="hl std">trainData</span> <span class="hl kwb">&lt;-</span> <span class="hl std">trainData[,</span> <span class="hl opt">-</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl num">6</span><span class="hl std">)]</span>
<span class="hl std">testData</span><span class="hl kwb">&lt;-</span> <span class="hl std">testData[,</span> <span class="hl opt">-</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl num">6</span><span class="hl std">)]</span>
<span class="hl kwa">for</span><span class="hl std">(i</span> <span class="hl kwa">in</span> <span class="hl num">1</span><span class="hl opt">:</span><span class="hl std">(</span><span class="hl kwd">length</span><span class="hl std">(trainData)</span><span class="hl opt">-</span><span class="hl num">1</span><span class="hl std">)){</span>
  <span class="hl std">trainData[,i]</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">as.numeric</span><span class="hl std">(trainData[,i])</span>
  <span class="hl std">testData[,i]</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">as.numeric</span><span class="hl std">(testData[,i])</span>
<span class="hl std">}</span>
<span class="hl com"># Converting the output variable into factor</span>
<span class="hl std">trainData[,</span><span class="hl num">53</span><span class="hl std">]</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">as.factor</span><span class="hl std">(trainData[,</span><span class="hl num">53</span><span class="hl std">])</span>
</pre></div>
</div></div>

<p>Cross Validation and Prediction

Spliting the data into trian and validation (70% and 30%) and then using training dataset applied random forest and other models and used validation dataset to predict.r</p>

<div class="chunk" id="unnamed-chunk-4"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># load the library</span>
<span class="hl kwd">library</span><span class="hl std">(caret)</span>
</pre></div>
<div class="warning"><pre class="knitr r">## Warning: package 'caret' was built under R version 3.1.3
</pre></div>
<div class="message"><pre class="knitr r">## Loading required package: lattice
## Loading required package: ggplot2
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(randomForest)</span>
</pre></div>
<div class="warning"><pre class="knitr r">## Warning: package 'randomForest' was built under R version 3.1.3
</pre></div>
<div class="message"><pre class="knitr r">## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(foreach)</span>
</pre></div>
<div class="warning"><pre class="knitr r">## Warning: package 'foreach' was built under R version 3.1.3
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(doParallel)</span>
</pre></div>
<div class="warning"><pre class="knitr r">## Warning: package 'doParallel' was built under R version 3.1.3
</pre></div>
<div class="message"><pre class="knitr r">## Loading required package: iterators
## Loading required package: parallel
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">workers</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">makeCluster</span><span class="hl std">(</span><span class="hl num">4</span><span class="hl std">)</span> <span class="hl com"># My computer has 4 cores</span>
<span class="hl kwd">registerDoParallel</span><span class="hl std">(workers)</span>

<span class="hl kwd">set.seed</span> <span class="hl std">(</span><span class="hl num">4224</span><span class="hl std">)</span>
<span class="hl std">inTrain</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">createDataPartition</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">=trainData</span><span class="hl opt">$</span><span class="hl std">classe,</span><span class="hl kwc">p</span><span class="hl std">=</span><span class="hl num">0.7</span><span class="hl std">,</span> <span class="hl kwc">list</span><span class="hl std">=</span><span class="hl num">FALSE</span><span class="hl std">)</span>
<span class="hl std">trainDataset</span> <span class="hl kwb">&lt;-</span> <span class="hl std">trainData[inTrain,]</span>
<span class="hl std">validation</span> <span class="hl kwb">&lt;-</span> <span class="hl std">trainData[</span><span class="hl opt">-</span><span class="hl std">inTrain,]</span>


<span class="hl com"># random forest algorithm on the training data set</span>
<span class="hl std">randfFit</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">randomForest</span><span class="hl std">(classe</span><span class="hl opt">~</span><span class="hl std">.,</span> <span class="hl kwc">data</span> <span class="hl std">= trainDataset,</span> <span class="hl kwc">method</span> <span class="hl std">=</span><span class="hl str">&quot;rf&quot;</span><span class="hl std">)</span>
<span class="hl std">randfFit</span>
</pre></div>
<div class="output"><pre class="knitr r">## 
## Call:
##  randomForest(formula = classe ~ ., data = trainDataset, method = &quot;rf&quot;) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.52%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 3903    1    0    0    2 0.0007680492
## B   16 2635    7    0    0 0.0086531226
## C    0    9 2382    5    0 0.0058430718
## D    0    0   23 2229    0 0.0102131439
## E    0    0    2    7 2516 0.0035643564
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl com"># Predict on validation data set</span>
<span class="hl std">randffPredtest</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">predict</span><span class="hl std">(randfFit, validation)</span>
<span class="hl com"># Predicted result</span>
<span class="hl kwd">confusionMatrix</span><span class="hl std">(randffPredtest, validation</span><span class="hl opt">$</span><span class="hl std">classe)</span>
</pre></div>
<div class="output"><pre class="knitr r">## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    6    0    0    0
##          B    0 1132    7    0    0
##          C    0    1 1019   10    0
##          D    0    0    0  953    0
##          E    0    0    0    1 1082
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9958          
##                  95% CI : (0.9937, 0.9972)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9946          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9939   0.9932   0.9886   1.0000
## Specificity            0.9986   0.9985   0.9977   1.0000   0.9998
## Pos Pred Value         0.9964   0.9939   0.9893   1.0000   0.9991
## Neg Pred Value         1.0000   0.9985   0.9986   0.9978   1.0000
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2845   0.1924   0.1732   0.1619   0.1839
## Detection Prevalence   0.2855   0.1935   0.1750   0.1619   0.1840
## Balanced Accuracy      0.9993   0.9962   0.9955   0.9943   0.9999
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl com"># predict on test</span>
<span class="hl std">randfPredtest</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">predict</span><span class="hl std">(randfFit, testData)</span>
<span class="hl std">randfPredtest</span>
</pre></div>
<div class="output"><pre class="knitr r">##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
</pre></div>
</div></div>


<p>Result:

Ramdom forest perfomred better with .5% OOB error rate on the training dataset and had a 99% accuracy in the test dataset.</p>


</body>
</html>
