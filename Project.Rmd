Practical Machine Learning
========================================================

Data Processing

After downloading the data, load the data and replace all the missing values and other corrupt values to NA and then find which fields have maximum NA and ignore the fields 

```{r}
trainData <- read.csv("pml-training.csv", header = TRUE, na.strings=c("","NA", "#DIV/0!"))
testData <- read.csv("pml-testing.csv", header = TRUE, na.strings=c("","NA", "#DIV/0!"))
```

Data cleaning

```{r}

NAPercent <- round(colMeans(is.na(trainData)), 2)
table(NAPercent)
FieldPosition <- which(NAPercent==0)[-1]
# remove the missinf values data
trainData <- trainData[, FieldPosition]
testData <- testData[, FieldPosition]
```

Converting the vairables to numeric anbd factor

```{r}
# Converting all the other dependant variables into numerical data to numeric class

trainData <- trainData[, -(1:6)]
testData<- testData[, -(1:6)]
for(i in 1:(length(trainData)-1)){
  trainData[,i] <- as.numeric(trainData[,i])
  testData[,i] <- as.numeric(testData[,i])
}
# Converting the output variable into factor
trainData[,53] <- as.factor(trainData[,53])
```


Cross Validation and Prediction

Spliting the data into trian and validation (70% and 30%) and then using training dataset applied random forest and other models and used validation dataset to predict.


```{r}
# load the library
library(caret)
library(randomForest)
library(foreach)
library(doParallel)
workers <- makeCluster(4) # My computer has 4 cores
registerDoParallel(workers)

set.seed (4224)
inTrain <- createDataPartition(y=trainData$classe,p=0.7, list=FALSE)
trainDataset <- trainData[inTrain,]
validation <- trainData[-inTrain,]


# random forest algorithm on the training data set
randfFit <- randomForest(classe~., data = trainDataset, method ="rf")
randfFit

# Predict on validation data set
randffPredtest <- predict(randfFit, validation)
# Predicted result
confusionMatrix(randffPredtest, validation$classe)

# predict on test
randfPredtest <- predict(randfFit, testData)
randfPredtest

)

```

Result:

Ramdom forest perfomred better with .5% OOB error rate on the training dataset and had a 99% accuracy in the test dataset.
